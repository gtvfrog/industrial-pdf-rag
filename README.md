# ğŸ“„ Industrial RAG Assistant

Sistema de perguntas e respostas baseado em PDFs de manuais industriais (WEG, WEG-CESTARI, Baldor).

---

## ğŸ—ï¸ Arquitetura

```
industrial-rag-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/                # Rotas FastAPI
â”‚   â”‚   â”œâ”€â”€ core/               # ConfiguraÃ§Ãµes
â”‚   â”‚   â”œâ”€â”€ services/           # ServiÃ§os (embeddings, retrieval, LLM, metrics)
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ scripts/                # UtilitÃ¡rios
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ Chat.py                 # Interface Streamlit
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ .env
â”‚   â””â”€â”€ logging.json
â”œâ”€â”€ k8s/                        # Manifests Kubernetes
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## âš™ï¸ Tecnologias

- **Backend**: FastAPI + Python 3.10+
- **Embeddings**: HuggingFace (multilingual-e5-base)
- **Vector Store**: FAISS ou InMemory
- **LLM**: Gemini 2.0 Flash ou Mistral 7B local
- **Frontend**: Streamlit
- **Deploy**: Docker + Kubernetes

---

## ğŸš€ Como rodar

### Local

```bash
# Backend
cd backend
pip install -r ../requirements.txt
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Frontend (em outro terminal)
streamlit run frontend/Chat.py --server.port 8501
```

### Docker

```bash
docker-compose up --build
```

- Backend: `http://localhost:8000/docs`
- Frontend: `http://localhost:8501`

### Kubernetes

```bash
# Configurar secrets
kubectl apply -f k8s/secret-example.yaml

# Deploy
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/backend-deployment.yaml
kubectl apply -f k8s/frontend-deployment.yaml
kubectl apply -f k8s/ingress.yaml
```

---

## ğŸ“Œ Endpoints Principais

### **POST /documents**
Upload e indexaÃ§Ã£o de PDFs

```bash
curl -X POST "http://localhost:8000/documents" \
  -F "files=@manual.pdf"
```

### **POST /question**
Realizar pergunta

```bash
curl -X POST http://localhost:8000/question \
  -H "Content-Type: application/json" \
  -d '{"question": "Como Ã© o transporte de redutores?"}'
```

### **GET /documents**
Listar documentos indexados

### **GET /metrics**
MÃ©tricas do sistema (retrieval, LLM, embeddings)

---

## ğŸ”§ ConfiguraÃ§Ã£o

Principais variÃ¡veis de ambiente (`.env`):

```bash
# LLM
LLM_PROVIDER=gemini                               # ou "local"
GEMINI_API_KEY=your_key_here
GEMINI_LLM_MODEL=gemini-2.0-flash-exp

# Embeddings
EMBEDDING_PROVIDER=huggingface
EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-base

# Vector Store
VECTOR_STORE_BACKEND=faiss                        # ou "inmemory"

# RAG
RAG_CHUNK_SIZE_CHARS=1000
RAG_CHUNK_OVERLAP_CHARS=150
ENABLE_QUERY_EXPANSION=true
QUERY_EXPANSION_USE_LLM=true
```

---

## ğŸ§ª Testes

```bash
pytest backend/tests/ -v
```

---

## ğŸ“ Funcionalidades

- âœ… Upload mÃºltiplo de PDFs
- âœ… Chunking inteligente com overlap
- âœ… Embeddings multilÃ­ngues
- âœ… Query expansion com LLM
- âœ… Multi-query retrieval
- âœ… Suporte a Gemini e LLMs locais
- âœ… MÃ©tricas detalhadas
- âœ… Interface web interativa
- âœ… Deploy com Docker e Kubernetes
