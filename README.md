# üìÑ RAG PDF System ‚Äì Document QA

Este projeto implementa um sistema simples e funcional para perguntas e respostas baseadas em PDFs.  
A ideia √© permitir que o usu√°rio envie documentos, o backend processe o conte√∫do e, depois, um LLM responda perguntas usando somente informa√ß√µes retiradas desses arquivos.

O foco √© clareza, modularidade e facilidade de execu√ß√£o ‚Äî alinhado ao que o desafio pede.

---

## üß† Vis√£o geral

O sistema funciona em tr√™s etapas principais:

1. **Envio de PDFs**  
   O backend recebe um ou mais arquivos, extrai texto, divide em chunks e gera embeddings.

2. **Indexa√ß√£o**  
   Os vetores s√£o armazenados em mem√≥ria (FAISS).  
   Simples, r√°pido e adequado ao escopo do desafio.

3. **Perguntas**  
   O usu√°rio envia uma pergunta ‚Üí o sistema busca os chunks mais relevantes ‚Üí monta o contexto ‚Üí passa para o LLM gerar a resposta.

O modelo pode ser local ou remoto. A implementa√ß√£o deixa isso flex√≠vel.

---

## üèóÔ∏è Arquitetura do Projeto

```
rag-pdf-system/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/           # Rotas e valida√ß√µes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/          # Configs e setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag/           # Chunking, embeddings, FAISS, retrieval
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm/           # Integra√ß√£o com LLM
‚îÇ   ‚îî‚îÄ‚îÄ main.py            # FastAPI entrypoint
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py   # Interface (opcional)
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ .env.example
‚îÇ   ‚îî‚îÄ‚îÄ settings.yaml
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Tecnologias

- **FastAPI** ‚Äì API r√°pida e tipada  
- **FAISS** ‚Äì Busca vetorial  
- **HuggingFace Embeddings** ‚Äì sentence-transformers  
- **LLM local ou API externa** ‚Äì Mistral, Ollama, OpenAI etc.  
- **Streamlit** (opcional) ‚Äì Interface visual simples  

---

## üìå Endpoints

### **POST /documents**
Recebe e indexa PDFs.

Exemplo:
```bash
curl -X POST "http://localhost:8000/documents" \
  -F "files=@manual1.pdf" \
  -F "files=@manual2.pdf"
```

Resposta:
```json
{
  "message": "Documents processed successfully",
  "documents_indexed": 2,
  "total_chunks": 128
}
```

---

### **POST /question**
Recebe uma pergunta e retorna resposta + refer√™ncias.

```bash
curl -X POST http://localhost:8000/question \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the power consumption?"}'
```

Resposta:
```json
{
  "answer": "The motor's power consumption is 2.3 kW.",
  "references": [
    "the motor xxx requires 2.3kw to operate at 60hz"
  ]
}
```

---

## üöÄ Como rodar

### **1. Instalar depend√™ncias**
```bash
pip install -r requirements.txt
```

### **2. Subir o backend**
```bash
uvicorn backend.main:app --reload
```

### **3. Opcional: rodar o Streamlit**
```bash
streamlit run frontend/streamlit_app.py
```

---

## üê≥ Docker

```bash
docker-compose up --build
```

Backend:  
`http://localhost:8000/docs`

Frontend opcional:  
`http://localhost:8501`

---

## üóÇÔ∏è Vector Store Backends

O sistema suporta dois backends de busca vetorial:

### **FAISS** (padr√£o)
- Usa FAISS IndexFlatIP
- Mais r√°pido para datasets grandes (>5K vetores)
- Suporta save/load com persist√™ncia de √≠ndice
- Otimizado para similaridade de cosseno

### **InMemory**
- Usa NumPy puro
- Ideal para desenvolvimento e datasets pequenos
- Suporta save/load em disco (pickle)
- Normaliza√ß√£o autom√°tica de vetores para cosine similarity

### Como trocar entre backends

Configure a vari√°vel de ambiente `VECTOR_STORE_BACKEND`:

```bash
# Usar FAISS (padr√£o)
export VECTOR_STORE_BACKEND=faiss

# Usar InMemory
export VECTOR_STORE_BACKEND=inmemory
```

Ou no `.env`:
```
VECTOR_STORE_BACKEND=faiss
```

### Benchmark de Performance

Para comparar a performance entre os backends:

```bash
python -m backend.scripts.benchmark_vector_stores
```

Exemplo de sa√≠da:
```
Backend        N           index_time_s    search_time_s   avg_search_ms
------------------------------------------------------------------------
InMemory       1000        0.12            0.05            0.50
Faiss          1000        0.08            0.02            0.20
InMemory       5000        0.58            0.23            2.30
Faiss          5000        0.35            0.08            0.80
InMemory       10000       1.15            0.45            4.50
Faiss          10000       0.68            0.15            1.50
```

O benchmark testa indexa√ß√£o e busca com vetores de 768 dimens√µes.

---

## üî™ Chunking Configuration

O sistema usa **chunking baseado em caracteres** para dividir os documentos em peda√ßos menores antes da indexa√ß√£o.

### Par√¢metros Padr√£o

- **chunk_size_chars**: 1000 caracteres
- **chunk_overlap_chars**: 150 caracteres

Esses valores foram escolhidos como um bom equil√≠brio entre:
- Contexto suficiente para embeddings significativos
- Tamanho gerenci√°vel para o modelo de linguagem
- Overlap adequado para manter continuidade entre chunks

### Configura√ß√£o

Os par√¢metros podem ser ajustados via:

**1. Vari√°veis de ambiente (`.env`):**
```bash
RAG_CHUNK_SIZE_CHARS=1000
RAG_CHUNK_OVERLAP_CHARS=150
```

**2. Arquivo de configura√ß√£o (`config/settings.yaml`):**
```yaml
rag:
  chunk_size_chars: 1000
  chunk_overlap_chars: 150
```

### Quando Ajustar

- **Documentos t√©cnicos densos**: Aumente `chunk_size` para ~1500 para manter contexto t√©cnico completo
- **Documentos com se√ß√µes curtas**: Reduza `chunk_size` para ~700 para evitar mistura de t√≥picos
- **Overlap**: Aumente para ~200 se houver muitas refer√™ncias cruzadas entre se√ß√µes

### Teste de Sanidade

Para verificar o comportamento do chunking:
```bash
python backend\scripts\test_chunking.py
```

---
